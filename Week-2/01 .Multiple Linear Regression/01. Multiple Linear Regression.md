# Multiple Linear Regression with Gradient Descent

## Overview

This repository implements **Multiple Linear Regression** using **Gradient Descent** for optimization. Multiple Linear Regression is a method used for predicting a continuous output based on multiple input features. The goal of this project is to provide a practical understanding of the concepts behind multiple linear regression, including the usage of **vectorization** and the **gradient descent** optimization technique, as taught in Andrew Ng's **Machine Learning** course on Coursera.

---

## Table of Contents

1. [Introduction](#introduction)
2. [Multiple Linear Regression](#multiple-linear-regression)
3. [Multiple Features](#multiple-features)
4. [Vectorization](#vectorization)
5. [Gradient Descent for Multiple Linear Regression](#gradient-descent-for-multiple-linear-regression)
6. [References](#references)

---

## Introduction

In this project, we implement the Multiple Linear Regression algorithm to predict an outcome based on multiple input features. The main objective is to minimize the cost function using the **gradient descent** algorithm. We also focus on optimizing the implementation by leveraging **vectorization**, which speeds up the calculations by taking advantage of matrix operations.

---

## Multiple Linear Regression

Multiple Linear Regression is an extension of simple linear regression where multiple input features are used to predict a target value. The hypothesis for multiple linear regression is:

$$
h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
$$

Where:

- $h_\theta(x)$ is the predicted value,
- $\theta_0, \theta_1, \dots, \theta_n$ are the model parameters (weights),
- $x_1, x_2, \dots, x_n$ are the input features.

The goal is to find the values of $\theta$ that minimize the **cost function** $J(\theta)$.

---

## Multiple Features

In multiple linear regression, the model uses more than one feature to make predictions. Given a dataset with $m$ training examples and $n$ features, we represent the data in matrix form:

- **X** is an $m \times n$ matrix of input features.
- **y** is a vector of the corresponding output values.
- **Î¸** is the parameter vector (size $n \times 1$).

The hypothesis for multiple features is:

$$
h_\theta(X) = X\theta
$$

Where:

- $X$ is the matrix of input features, including the bias term (intercept),
- $\theta$ is the parameter vector,
- $h_\theta(X)$ is the vector of predictions.

---

## Vectorization

Vectorization is the technique of representing the data and calculations using matrices and vectors, rather than loops, to perform efficient computations. By using vectorized operations, we can drastically improve the performance and reduce the computation time.

The cost function $J(\theta)$ in vectorized form is:

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
$$

In vectorized form, this becomes:

$$
J(\theta) = \frac{1}{2m} (X\theta - y)^T (X\theta - y)
$$

Similarly, the gradient for gradient descent is:

$$
\text{grad} = \frac{1}{m} X^T (X\theta - y)
$$

Where:

- $X$ is the matrix of input features,
- $y$ is the vector of output values,
- $\theta$ is the parameter vector.

---

## Gradient Descent for Multiple Linear Regression

The goal of **Gradient Descent** is to minimize the cost function $J(\theta)$. In this case, we use the gradient descent algorithm to update the values of $\theta$ iteratively.

The gradient descent update rule is:

$$
\theta := \theta - \alpha \frac{1}{m} X^T (X\theta - y)
$$

Where:

- $\alpha$ is the learning rate (controls the step size),
- $X$ is the matrix of input features,
- $\theta$ is the parameter vector,
- $y$ is the vector of output values.

---

## References

- **Andrew Ng's Machine Learning Course** (Coursera): [Link to Course](https://www.coursera.org/learn/machine-learning)
- **Multiple Linear Regression Overview**: [Stanford University CS229 Lecture Notes](http://cs229.stanford.edu/)
