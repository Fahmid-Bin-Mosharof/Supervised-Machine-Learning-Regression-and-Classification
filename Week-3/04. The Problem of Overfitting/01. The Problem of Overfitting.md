# The Problem of Overfitting

---

## 1. The Problem of Overfitting

**Overfitting** occurs when a model learns not only the underlying pattern in the training data, but also the noise. As a result, it performs well on the training set but poorly on unseen data.

**Symptoms of overfitting:**

- Very low training error
- High error on cross-validation or test data
- Complex hypothesis (e.g., high-degree polynomial)

Example:

- A high-degree polynomial may perfectly fit all training points but generalizes poorly to new data.

---

## 2. Addressing Overfitting

To reduce overfitting:

- **Simplify the model** (e.g., use fewer features or lower-degree polynomials)
- **Get more training data**
- **Regularize** the model

Regularization is the most common technique. It works by adding a penalty to the cost function for having large model parameters.

---

## 3. Cost Function with Regularization

The idea of regularization is to keep model parameters small by adding a penalty term to the original cost function.

### Regularized Cost Function:

$$
J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{n} \theta_j^2 \right]
$$

- $m$ = number of training examples
- $\theta_j$ = model parameters (excluding $\theta_0$)
- $\lambda$ = regularization parameter (controls the amount of penalty)

---

## 4. Regularized Linear Regression

For linear regression, the hypothesis remains:

$$
h_\theta(x) = \theta^T x
$$

The regularized cost function becomes:

$$
J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^{m} (\theta^T x^{(i)} - y^{(i)})^2 + \lambda \sum_{j=1}^{n} \theta_j^2 \right]
$$

Note: $\theta_0$ (the bias term) is **not** regularized.

---

## 5. Regularized Logistic Regression

For logistic regression, the hypothesis is:

$$
h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}
$$

The regularized cost function is:

$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2
$$

This helps prevent the logistic regression model from overfitting by penalizing large weights.

---

### ðŸ“š Reference

This content is derived from Andrew Ngâ€™s Coursera course:  
[Supervised Machine Learning: Regression and Classification](https://www.coursera.org/learn/machine-learning)
