## Cost Function for Logistic Regression

In Logistic Regression, the **hypothesis function** is defined as:

$$
h\_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}
$$

This outputs a probability between 0 and 1, which can be interpreted as the probability that the output is 1, given input features \( x \).

To measure the performance of our hypothesis, we use the **Log Loss (logistic loss)** cost function:

$$
J(\theta) = -\frac{1}{m} \sum*{i=1}^{m} \left[ y^{(i)} \log(h*\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h\_\theta(x^{(i)})) \right]
$$

This function penalizes confident but wrong predictions heavily, ensuring a smooth optimization landscape for gradient descent.

---

## Simplified Cost Function for Logistic Regression

To understand the cost function intuitively, we often look at it for individual training examples. The cost function for a single example is:

$$
\text{Cost}(h_\theta(x), y) = -y \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))
$$

This form makes it clear that:

- When \( y = 1 \), we want \( h\_\theta(x) \) close to 1.
- When \( y = 0 \), we want \( h\_\theta(x) \) close to 0.

---

### Final Combined Form

The final combined form for a single training example is:

$$
\text{Cost}(h*\theta(x), y) = -y \log(h*\theta(x)) - (1 - y) \log(1 - h\_\theta(x))
$$

This allows us to avoid writing two separate cases and makes vectorization easier for implementation.

---

### Reference

This content is derived from Andrew Ngâ€™s Coursera course:  
[Supervised Machine Learning: Regression and Classification](https://www.coursera.org/learn/machine-learning)
